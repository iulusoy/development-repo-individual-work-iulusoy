{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of the analysis notebook\n",
    "\n",
    "**Author : Inga Ulusoy**  \n",
    "*Date : 06/25*  \n",
    "*Affiliation : SSC*  \n",
    "\n",
    "Place the required modules in the top, followed by required constants and global functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and global functions/variables\n",
    "data_file_path = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading of the data files\n",
    "def read_pandas(path, name):\n",
    "    file = path / name\n",
    "    return pd.read_csv(file, sep=r\"\\s+\")  # noqa\n",
    "\n",
    "\n",
    "def read_numpy(path, name, skip=1):\n",
    "    file = path / name\n",
    "    return np.loadtxt(file, skiprows=skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis\n",
    "\n",
    "Find correlations in the data sets. Analyse the data statistically and plot your results.  \n",
    "\n",
    "Here we would want to do everything with pandas and leave the data in a dataframe. The files that are relevant to you are `expect.t`, `npop.t` and `table.dat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Read in expec.t and plot relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and plot expec.t\n",
    "data_expec = read_pandas(data_file_path, \"expec.t\")\n",
    "data_expec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_expec.plot(x=\"time\", y=\"<z>\")\n",
    "plt.savefig(\"expec_t.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.relplot(data=data_expec, kind=\"line\", x=\"time\", y=\"<z>\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.pairplot(data_expec, corner=True)\n",
    "plt.savefig(\"pairplot_expec_t.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can discard the entries norm, \\<x>, and \\<y> as these are mostly constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate columns based on the variance: if the variance of the values\n",
    "# in a column is below a given threshold, that column is discarded\n",
    "data_expec.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 1.0e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_expec.drop(df_expec.var()[df_expec.var()<threshv].index.values, axis=1)\n",
    "# print(df_expec.var()<threshv) # this will return columns with variance below the threshold\n",
    "# df_expec.var()[df_expec.var()<threshv].index # this will return the index of those columns\n",
    "# df_expec.var()[df_expec.var()<threshv].index.values # this will return the index of those columns in an array\n",
    "# df_expec.drop(df_expec.var()[df_expec.var()<threshv].index.values) # we want to drop those columns\n",
    "data_expec.drop(\n",
    "    data_expec.var()[data_expec.var() < thresh].index.values, axis=1\n",
    ")  # but we have to pick the right axis (columns, not rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_significant(data_in, thresh):\n",
    "    data_out = data_in.drop(data_in.var()[data_in.var() < thresh].index.values, axis=1)\n",
    "    indices = data_in.var()[data_in.var() > thresh].index.values\n",
    "    return data_out, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expec, indices = check_if_significant(data_expec, thresh)  # flake8-noqa-cell\n",
    "display(df_expec.head())\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Create plots of the relevant data and save as .pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plots\n",
    "# I am leaving this to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Read in file `npop.t` and analyze correlations in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in npop.t\n",
    "data_npop = read_pandas(data_file_path, \"npop.t\")\n",
    "data_npop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8-noqa-cell\n",
    "# discard all columns with variance below a set threshold - we can consider them as constant\n",
    "df_npop, indices_npop = check_if_significant(data_npop, thresh)\n",
    "display(df_npop.head())\n",
    "print(indices_npop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the remaining columns. Seaborn prefers \"long format\" (one column for all measurement values, one column to indicate the type) as input, whereas the cvs is in \"wide format\" (one column per measurement type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ideally with seaborn\n",
    "sn.lineplot(x=\"time\", y=\"value\", hue=\"variable\", data=pd.melt(df_npop, [\"time\"]))\n",
    "# Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.\n",
    "plt.savefig(\"npop_t.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify the pairwise correlation in the data\n",
    "\n",
    "- negative correlation: y values decrease for increasing x - large values of one feature correspond to small values of the other feature\n",
    "- weak or no correlation: no trend observable, association between two features is hardly observable\n",
    "- positive correlation: y values increase for decreasing x - small values of one feature correspond to small values of the other feature\n",
    "\n",
    "Remember that correlation does not indicate causation - the reason that two features are associated can lie in their dependence on same factors.\n",
    "\n",
    "Correlate the value pairs using Pearson's $r$. Pearson's $r$ is a measure of the linear relationship between features:\n",
    "\n",
    "$r = \\frac{\\sum_i(x_i − \\bar{x})(y_i − \\bar{y})}{\\sqrt{\\sum_i(x_i − \\bar{x})^2 \\sum_i(y_i − \\bar{y})^2}}$\n",
    "\n",
    "Here, $\\bar{x}$ and $\\bar{y}$ indicate mean values. $i$ runs over the whole data set. For a positive correlation, $r$ is positive, and negative for a negative correlation, with minimum and maximum values of -1 and 1, indicating a perfectly linear relationship. Weakly or not correlated features are characterized by $r$-values close to 0.\n",
    "\n",
    "Other measures of correlation that can be used are Spearman's rank (value pairs follow monotonic function) or Kendall's $\\tau$ (measures ordinal association), but they do not apply here. You can also define measures yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the correlation matrix\n",
    "print(\"Correlation Matrix\")\n",
    "print(df_npop.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal values tell us that each value is perfectly correlated with itself. We are not interested in the diagonal values and also not in the correlation with time. We also need to get rid of redundant entries. Finally, we need to find the value pairs that exhibit the highest linear correlation. We still want to know if it is positive or negative correlation, so we cannot get rid of the sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8-noqa-cell\n",
    "# get rid of time column, lower triangular and diagonal entries of the correlation matrix\n",
    "# sort the remaing values according to their absolute value, but keep the sign\n",
    "def get_correlation_measure(df):\n",
    "    drop_values = set()  # an unordered collection of items\n",
    "    cols = df.columns  # get the column labels\n",
    "    print(cols)\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(\n",
    "            0, i + 1\n",
    "        ):  # get rid of all diagonal entries and the lower triangular\n",
    "            drop_values.add((cols[i], cols[j]))\n",
    "    print(drop_values)\n",
    "    return drop_values\n",
    "\n",
    "\n",
    "df_npop_short = df_npop.drop([\"time\"], axis=1)  # get rid of time column\n",
    "drop_vals = get_correlation_measure(\n",
    "    df_npop_short\n",
    ")  # get rid of lower triangular and diagonal entries\n",
    "corr2 = df_npop_short.corr().unstack()  # pivot the correlation matrix\n",
    "# sort by absolute values but keep sign\n",
    "corr2 = corr2.drop(labels=drop_vals).sort_values(\n",
    "    ascending=False, key=lambda col: col.abs()\n",
    ")\n",
    "display(corr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the entries in the left column are not repeated if they do not change from the row above (so the fourth feature pair is MO3 and MO6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Print the resulting data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Calculate the Euclidean distance (L2 norm) for the vectors in `table.dat`\n",
    "\n",
    "\n",
    "The Euclidean distance measures the distance between to objects that are not points:\n",
    "\n",
    "$d(p,q) = \\sqrt{\\left(p-q\\right)^2}$\n",
    "\n",
    "In this case, consider each of the columns in table.dat as a vector in Euclidean space, where column $r(x)$ and column $v(x)$ denote a pair of vectors that should be compared, as well as $r(y)$ and $v(y)$, and r(z) and v(z).\n",
    "\n",
    "(Background: These are dipole moment components in different gauges, the length and velocity gauge.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in table.dat - I suggest reading it as a numpy array\n",
    "data_table = read_numpy(data_file_path, \"table.dat\")\n",
    "data_table[0]\n",
    "# replace the NaNs by zero\n",
    "data_table = np.nan_to_num(data_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate how different the vectors in column 2 are from column 3, column 4 from column 5, and column 6 from column 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Euclidean distance\n",
    "def euclidean_distance(list_ref, list_comp, vectors):\n",
    "    distances = np.zeros(len(list_ref))\n",
    "    for i in range(len(list_ref)):\n",
    "        distances[i] = np.linalg.norm(vectors[list_comp[i]] - vectors[list_ref[i]])\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result and save to a .pdf\n",
    "out_dist = euclidean_distance([2, 4, 6], [3, 5, 7], data_table)\n",
    "print(out_dist)\n",
    "x = range(0, len(out_dist))\n",
    "plt.bar(x, out_dist)\n",
    "plt.xticks(x, (\"x\", \"y\", \"z\"))\n",
    "plt.savefig(\"euclidean_distance.pdf\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical analysis\n",
    "\n",
    "Analyze the data using autocorrelation functions and discrete Fourier transforms. Plot your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some global functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Read in `efield.t` and Fourier-transform relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and plot efield.t\n",
    "efield = read_numpy(data_file_path, \"efield.t\", skip=1)\n",
    "efield = efield.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are interested in column 2 since the others are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard the columns with variance below threshold - these are considered constant\n",
    "print(np.var(efield))  # var computed for the overall, flattened array\n",
    "print(np.var(efield, axis=1))  # var computed for each column (row-major order)\n",
    "print(\n",
    "    np.nonzero(np.var(efield, axis=1) > 1e-4)\n",
    ")  # returns the indices of values for which condition applies\n",
    "print(\n",
    "    efield[np.nonzero(np.var(efield, axis=1) > 1e-4)]\n",
    ")  # get the numpy array for those indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_significant_np(data_in, thresh):\n",
    "    indices = np.nonzero(np.var(data_in, axis=1) > thresh)\n",
    "    data_out = data_in[indices]\n",
    "    return data_out, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the FT\n",
    "def do_DFT(data, tmax):\n",
    "    data_s = np.fft.rfft(data)\n",
    "    data_w = np.fft.rfftfreq(tmax)\n",
    "    return data_s, data_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efield2, indices = check_if_significant_np(efield, thresh=1e-4)\n",
    "print(efield2)\n",
    "plt.plot(efield2[1])\n",
    "plt.savefig(\"efield2.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete Fourier transform of the remaining column: You only need the real frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efield_s, frequency = do_DFT(efield2[1], len(efield2[0]))\n",
    "plt.plot(frequency, abs(efield_s) ** 2)  # the frequency of this laser pulse was 0.116\n",
    "plt.savefig(\"efield_spectrum.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Generate a plot of your results to be saved as pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Calculate the autocorrelation function from nstate_i.t\n",
    "The autocorrelation function measures how correlated subsequent vectors are with an initial vector; ie. \n",
    "\n",
    "$\\Psi_{corr} = \\langle \\Psi(t=0) | \\Psi(t) \\rangle = \\int_0^{tfin} \\Psi(0)^* \\Psi(t) dt$\n",
    "\n",
    "Since we are in a numerical representation, the integral can be replaced with a sum; and the given vectors are already normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in as numpy array\n",
    "wavef = read_numpy(data_file_path, \"nstate_i.t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the time column (column 0) in a vector and drop from array\n",
    "time = wavef[0]\n",
    "wavef = np.delete(wavef, [0], axis=0)\n",
    "print(wavef[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the data representation: this is in fact a complex matrix\n",
    "# the real part of each matrix column is contained in numpy array column 0, 2, 4, 6, ...\n",
    "# the imaginary part of each matrix column is contained in numpy array column 1, 3, 5, 7, ...\n",
    "# convert the array that was read as dtype=float into a dtype=complex array\n",
    "# convert to complex array\n",
    "realpart = wavef[0::2]\n",
    "imagpart = wavef[1::2]\n",
    "wavefc = realpart + 1j * imagpart\n",
    "print(wavefc[0])\n",
    "print(realpart[0])\n",
    "print(imagpart[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the autocorrelation function, we want the overlap between the first vector at time 0 and all\n",
    "# subsequent vectors at later times - the sum of the product of initial and subsequent vectors for each time step\n",
    "def calc_auto(wavef):\n",
    "    aucofu = np.zeros(len(wavef[0]), dtype=complex)\n",
    "    for i in range(0, len(wavef[0])):\n",
    "        aucofu[i] = np.sum(wavef[:, 0] * np.conjugate(wavef[:, i]))\n",
    "    return aucofu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucofu = calc_auto(wavefc)\n",
    "print(aucofu)\n",
    "plt.plot(abs(aucofu**2))\n",
    "plt.savefig(\"autocorrelation_function.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Generate a plot of your results to be saved as pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the autocorrelation function - real, imaginary and absolute part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Discrete Fourier transform of the autocorrelation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8-noqa-cell\n",
    "# discrete Fourier-transform the autocorrelation function - now we need all frequency components,\n",
    "# also the negative ones\n",
    "# now do the FT\n",
    "# do the FT - see https://numpy.org/doc/stable/reference/routines.fft.html\n",
    "def do_fft(data, tmax):\n",
    "    data_s = np.fft.fft(data)\n",
    "    data_w = np.fft.fftfreq(tmax)\n",
    "    # only take the positive frequency components\n",
    "    return data_w[0 : tmax // 2], data_s[0 : tmax // 2]\n",
    "\n",
    "\n",
    "energy, spec = do_fft(aucofu, len(time))\n",
    "print(energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Generate a plot of your results to be saved as pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the power spectrum (abs**2)\n",
    "# plt.plot(energy,real(spec))\n",
    "# plt.plot(energy,imag(spec))\n",
    "# plt.plot(energy,abs(spec))\n",
    "plt.plot(energy, abs(spec) ** 2)\n",
    "plt.savefig(\"autocorrelation_spectrum.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssd-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
